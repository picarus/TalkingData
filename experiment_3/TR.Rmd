---
title: "R Notebook"
output: html_notebook
---

```{r}
library(pacman)
p_load(ggplot2) # Data visualization
p_load(readr) # CSV file I/O, e.g. the read_csv function
p_load(dplyr)
# devtools::install_github("hadley/multidplyr")
library(multidplyr)
p_load(parallel)
p_load(fasttime)
p_load(xgboost)
p_load(mlr)
p_load(mlrMBO)
p_load(caret)
p_load(magrittr)
p_load(parallelMap)
p_load(DiceKriging)
p_load(rgenoud)

```





```{r}
print(n_cores <- detectCores()-1)
```

# train, validation 

```{r}
table(valid_cv$is_attributed)
```

```{r}
table(train_cv$is_attributed)
```

```{r}

train_cv %>% head(100)
```


```{r}
set.seed(0)
```


```{r}
params <- getParamSet("classif.xgboost")
params
```



```{r}

event_rate <- 0.002

child_weight <- 1/sqrt(event_rate)

xgb_param <- makeParamSet(
  makeNumericParam("gamma", lower= -2, upper = 0, trafo = function(x) 10^x),
  makeIntegerParam("max_depth", lower = 3, upper = 10),  # number of splits in each tree
  makeNumericParam("eta", lower = .1, upper = .8), # "shrinkage" - prevents overfitting
  makeNumericParam('min_child_weight', lower=child_weight/2, upper=child_weight*2),
  makeNumericParam("colsample_bytree", lower=0.3 , upper=0.6),
  makeNumericParam("colsample_bylevel", lower=0.7 , upper=1),
  makeNumericParam("lambda", lower = -1, upper = 0, trafo = function(x) 10^x) # L2 regularization - prevents overfitting
)
```

```{r}
# mlrMBO part
mboControl <- makeMBOControl()
mboControl <- setMBOControlTermination(mboControl, iters = 1, time.budget=18000)  
mboControl <- setMBOControlInfill(mboControl, crit = makeMBOInfillCritAEI()) # augmented expected improvement
# mlr part
tuneMBOControl <- makeTuneControlMBO(mbo.control = mboControl)
```


## Dynamic setup


```{r}
trainTask <- makeClassifTask(data = train_cv %>% select(-wghts), target = "is_attributed", positive = 1)
```

```{r}

train_cv_matrix <- xgboost::xgb.DMatrix(data=data.matrix(train_cv %>% select(-is_attributed, -wghts)),
                                        label=train_cv$is_attributed, missing = NA, weight=train_cv$wghts)

rm(train_cv);gc()

```

```{r}

valid_cv_matrix <- xgboost::xgb.DMatrix(data=data.matrix(valid_cv %>% select(-is_attributed, -wghts)),
                                        label=valid_cv$is_attributed, missing = NA, weight=valid_cv$wghts)

rm(valid_cv);gc()
```

```{r}

xgb <- makeLearner( "classif.xgboost", 
                    predict.type = "prob",
                    par.vals = list(
                      objective = "binary:logistic",
                      eval_metric = "auc",
                      subsample = 1,
                      watchlist = list( train = train_cv_matrix, val = valid_cv_matrix),
                      nrounds = 100, 
                      maximize = T,
                      verbose = 1, 
                      nthread = n_cores,
                      print_every_n = 5,
                      early_stopping_rounds = 5)
                    )

```




```{r}
# xgbSimple <- setHyperPars(xgb, par.vals = resTune$x)
# 
# xgb_model_simple <- train(xgbSimple, trainTask)
# 
# save(xgb_model_simple, file="xgb_model_simple.RDS")
# 
# xgb_model <- xgb_model_simple
```


```{r}

resTune <- tuneParams(learner=xgb, 
                task=trainTask, 
                resampling=makeResampleDesc("Subsample", iters = 2, split = 0.95),
                # TODO: PROVIDE resample.fun
                measures=auc, 
                par.set = xgb_param, 
                control = tuneMBOControl, 
                show.info = T)
```

```{r}

save(resTune, file="xgb.tune.RDS")

xgb_tuned_learner <- setHyperPars(learner = xgb, par.vals = resTune$x)

xgb_model <- train(xgb_tuned_learner, trainTask)

save(xgb_model, file="xgb_model.RDS")
```


 